{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4868,
     "status": "ok",
     "timestamp": 1599295316162,
     "user": {
      "displayName": "Kaisei Hara",
      "photoUrl": "",
      "userId": "00093271953957882617"
     },
     "user_tz": -540
    },
    "id": "D3apNnoc6Mnn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc\n",
    "import re\n",
    "from pytorch_memlab import MemReporter\n",
    "from torchsummary import summary\n",
    "# ここから自作\n",
    "import model\n",
    "import result\n",
    "import mode\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29932,
     "status": "ok",
     "timestamp": 1599295341242,
     "user": {
      "displayName": "Kaisei Hara",
      "photoUrl": "",
      "userId": "00093271953957882617"
     },
     "user_tz": -540
    },
    "id": "DAmdrdrJws83"
   },
   "outputs": [],
   "source": [
    "# 時刻を表示してくれるようになるprint関数のwrapper\n",
    "def datePrint(*args, **kwargs):\n",
    "    from datetime import datetime\n",
    "    print(datetime.now().strftime('[%Y/%m/%d %H:%M:%S] '), end=\"\")\n",
    "    print(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  datePrint(\"loading pickle data\")\n",
    "# input_val0 = pickle.load(open(\"../data/dna_len5_random256/input_test0.pkl\",\"rb\"))\n",
    "# target_val0 = pickle.load(open(\"../data/dna_len5_random256/target_test0.pkl\",\"rb\")) #256のみ\n",
    "# target_val0 = torch.flip(target_val0, dims=[1])\n",
    "# input_train0 = pickle.load(open(\"../data/dna_len5_random256/input_train0.pkl\",\"rb\"))\n",
    "# target_train0 = pickle.load(open(\"../data/dna_len5_random256/target_train0.pkl\",\"rb\")) #256以下\n",
    "# target_train0 = torch.flip(target_train0, dims=[1])\n",
    "\n",
    "# input_all = torch.cat([input_train0, input_val0], dim=0)\n",
    "# target_all = torch.cat([target_train0, target_val0], dim=0)\n",
    "# dataset = model.Dataset(input_all, target_all)\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [900000, 100000])\n",
    "\n",
    "# del input_val0, target_val0, input_train0, target_train0\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datePrint(\"loading pickle data\")\n",
    "# input_val0 = pickle.load(open(\"../data/max_span100_256/input_val0.pkl\",\"rb\"))\n",
    "# target_val0 = pickle.load(open(\"../data/max_span100_256/target_val0.pkl\",\"rb\")) #256のみ\n",
    "# target_val0 = torch.flip(target_val0, dims=[1])\n",
    "# input_train0 = pickle.load(open(\"../data/max_span100_256/input_train0.pkl\",\"rb\"))\n",
    "# target_train0 = pickle.load(open(\"../data/max_span100_256/target_train0.pkl\",\"rb\")) #256以下\n",
    "# target_train0 = torch.flip(target_train0, dims=[1])\n",
    "# input_val1 = pickle.load(open(\"../data/max_span100_256/input_val1.pkl\",\"rb\"))\n",
    "# target_val1 = pickle.load(open(\"../data/max_span100_256/target_val1.pkl\",\"rb\")) #256のみ\n",
    "# target_val1 = torch.flip(target_val1, dims=[1])\n",
    "# input_train1 = pickle.load(open(\"../data/max_span100_256/input_train1.pkl\",\"rb\"))\n",
    "# target_train1 = pickle.load(open(\"../data/max_span100_256/target_train1.pkl\",\"rb\")) #256以下\n",
    "# target_train1 = torch.flip(target_train1, dims=[1])\n",
    "\n",
    "# input_all = torch.cat([input_train0, input_val0, input_train1, input_val1], dim=0)\n",
    "# target_all = torch.cat([target_train0, target_val0, target_train1, target_val1], dim=0)\n",
    "# dataset = model.Dataset(input_all, target_all)\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [1800000, 200000])\n",
    "\n",
    "# del input_val0, target_val0, input_train0, target_train0, input_val1, target_val1, input_train1, target_train1\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021/05/21 13:36:03] loading pickle data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datePrint(\"loading pickle data\")\n",
    "input_val0 = pickle.load(open(\"../data/max_span100_512/input_val0.pkl\",\"rb\"))\n",
    "target_val0 = pickle.load(open(\"../data/max_span100_512/target_val0.pkl\",\"rb\")) #512のみ\n",
    "target_val0 = torch.flip(target_val0, dims=[1])\n",
    "input_train0 = pickle.load(open(\"../data/max_span100_512/input_train0.pkl\",\"rb\"))\n",
    "target_train0 = pickle.load(open(\"../data/max_span100_512/target_train0.pkl\",\"rb\")) #512以下\n",
    "target_train0 = torch.flip(target_train0, dims=[1])\n",
    "input_val1 = pickle.load(open(\"../data/max_span100_512/input_val1.pkl\",\"rb\"))\n",
    "target_val1 = pickle.load(open(\"../data/max_span100_512/target_val1.pkl\",\"rb\")) #512のみ\n",
    "target_val1 = torch.flip(target_val1, dims=[1])\n",
    "input_train1 = pickle.load(open(\"../data/max_span100_512/input_train1.pkl\",\"rb\"))\n",
    "target_train1 = pickle.load(open(\"../data/max_span100_512/target_train1.pkl\",\"rb\")) #512以下\n",
    "target_train1 = torch.flip(target_train1, dims=[1])\n",
    "# input_val2 = pickle.load(open(\"../data/max_span100_512/input_val2.pkl\",\"rb\"))\n",
    "# target_val2 = pickle.load(open(\"../data/max_span100_512/target_val2.pkl\",\"rb\")) #512のみ\n",
    "# target_val2 = torch.flip(target_val2, dims=[1])\n",
    "# input_train2 = pickle.load(open(\"../data/max_span100_512/input_train2.pkl\",\"rb\"))\n",
    "# target_train2 = pickle.load(open(\"../data/max_span100_512/target_train2.pkl\",\"rb\")) #512以下\n",
    "# target_train2 = torch.flip(target_train2, dims=[1])\n",
    "# input_val3 = pickle.load(open(\"../data/max_span100_512/input_val3.pkl\",\"rb\"))\n",
    "# target_val3 = pickle.load(open(\"../data/max_span100_512/target_val3.pkl\",\"rb\")) #512のみ\n",
    "# target_val3 = torch.flip(target_val3, dims=[1])\n",
    "# input_train3 = pickle.load(open(\"../data/max_span100_512/input_train3.pkl\",\"rb\"))\n",
    "# target_train3 = pickle.load(open(\"../data/max_span100_512/target_train3.pkl\",\"rb\")) #512以下\n",
    "# target_train3 = torch.flip(target_train3, dims=[1])\n",
    "\n",
    "# input_all = torch.cat([input_train0, input_val0, input_train1, input_val1, input_train2, input_val2, input_train3, input_val3], dim=0)\n",
    "# target_all = torch.cat([target_train0, target_val0, target_train1, target_val1, target_train2, target_val2, target_train3, target_val3], dim=0)\n",
    "input_all = torch.cat([input_train0, input_val0, input_train1, input_val1], dim=0)\n",
    "target_all = torch.cat([target_train0, target_val0, target_train1, target_val1], dim=0)\n",
    "dataset = model.Dataset(input_all, target_all)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [1800000, 200000])\n",
    "\n",
    "del input_val0, target_val0, input_train0, target_train0, input_val1, target_val1, input_train1, target_train1\n",
    "# del input_val2, target_val2,  input_train2, target_train2, input_val3, target_val3, input_train3, target_train3\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def lambda_epoch(epoch):\n",
    "    # スケジューラの設定\n",
    "    max_epoch = 20\n",
    "    return math.pow((1-epoch/max_epoch), 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "4d55df15f3f947888809d424885dfe55",
      "a3ed74bd975e4e87955a0de33c520916",
      "8a817f317b5c42f592f130f576ee3063",
      "3c5b3c652eab41459f3779503ab44717",
      "de9d1fa4cee24559a816afe706b91081",
      "5098cf4e3a6a4e5fb9daec2f6dc59c9d",
      "2ee0ab1bbc1e4cada381cdadbc3dc5f2",
      "16b98715895a42739075b7711e0ba437"
     ]
    },
    "colab_type": "code",
    "id": "FK5eCRTe6MoS",
    "outputId": "ae6267ee-0394-44aa-e1bd-0af90e62456d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021/05/21 13:36:37] 0 nani 0 layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaisei-h/.local/lib/python3.7/site-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/home/kaisei-h/.local/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:125: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1               [-1, 512, 5]              25\n",
      "            Conv1d-2              [-1, 16, 512]             896\n",
      "       BatchNorm1d-3              [-1, 16, 512]              32\n",
      "              ReLU-4              [-1, 16, 512]               0\n",
      "conv1DBatchNormRelu-5              [-1, 16, 512]               0\n",
      "            Conv1d-6              [-1, 16, 512]           2,832\n",
      "       BatchNorm1d-7              [-1, 16, 512]              32\n",
      "              ReLU-8              [-1, 16, 512]               0\n",
      "conv1DBatchNormRelu-9              [-1, 16, 512]               0\n",
      "           Conv1d-10              [-1, 32, 512]           5,664\n",
      "      BatchNorm1d-11              [-1, 32, 512]              64\n",
      "             ReLU-12              [-1, 32, 512]               0\n",
      "conv1DBatchNormRelu-13              [-1, 32, 512]               0\n",
      "        MaxPool1d-14              [-1, 32, 256]               0\n",
      "           Conv1d-15              [-1, 32, 256]          11,296\n",
      "      BatchNorm1d-16              [-1, 32, 256]              64\n",
      "             ReLU-17              [-1, 32, 256]               0\n",
      "conv1DBatchNormRelu-18              [-1, 32, 256]               0\n",
      "           Conv1d-19              [-1, 64, 256]          22,592\n",
      "      BatchNorm1d-20              [-1, 64, 256]             128\n",
      "             ReLU-21              [-1, 64, 256]               0\n",
      "conv1DBatchNormRelu-22              [-1, 64, 256]               0\n",
      "        MaxPool1d-23              [-1, 64, 128]               0\n",
      "           Conv1d-24              [-1, 64, 128]          45,120\n",
      "      BatchNorm1d-25              [-1, 64, 128]             128\n",
      "             ReLU-26              [-1, 64, 128]               0\n",
      "conv1DBatchNormRelu-27              [-1, 64, 128]               0\n",
      "           Conv1d-28             [-1, 128, 128]          90,240\n",
      "      BatchNorm1d-29             [-1, 128, 128]             256\n",
      "             ReLU-30             [-1, 128, 128]               0\n",
      "conv1DBatchNormRelu-31             [-1, 128, 128]               0\n",
      "        MaxPool1d-32              [-1, 128, 64]               0\n",
      "           Conv1d-33              [-1, 128, 64]         180,352\n",
      "      BatchNorm1d-34              [-1, 128, 64]             256\n",
      "             ReLU-35              [-1, 128, 64]               0\n",
      "conv1DBatchNormRelu-36              [-1, 128, 64]               0\n",
      "           Conv1d-37              [-1, 128, 64]         180,352\n",
      "      BatchNorm1d-38              [-1, 128, 64]             256\n",
      "             ReLU-39              [-1, 128, 64]               0\n",
      "conv1DBatchNormRelu-40              [-1, 128, 64]               0\n",
      "           Conv1d-41              [-1, 256, 64]         360,704\n",
      "      BatchNorm1d-42              [-1, 256, 64]             512\n",
      "             ReLU-43              [-1, 256, 64]               0\n",
      "conv1DBatchNormRelu-44              [-1, 256, 64]               0\n",
      "        MaxPool1d-45              [-1, 256, 32]               0\n",
      "           Conv1d-46              [-1, 256, 32]         721,152\n",
      "      BatchNorm1d-47              [-1, 256, 32]             512\n",
      "             ReLU-48              [-1, 256, 32]               0\n",
      "conv1DBatchNormRelu-49              [-1, 256, 32]               0\n",
      "           Conv1d-50              [-1, 256, 32]         721,152\n",
      "      BatchNorm1d-51              [-1, 256, 32]             512\n",
      "             ReLU-52              [-1, 256, 32]               0\n",
      "conv1DBatchNormRelu-53              [-1, 256, 32]               0\n",
      "           Conv1d-54              [-1, 512, 32]       1,442,304\n",
      "      BatchNorm1d-55              [-1, 512, 32]           1,024\n",
      "             ReLU-56              [-1, 512, 32]               0\n",
      "conv1DBatchNormRelu-57              [-1, 512, 32]               0\n",
      "        MaxPool1d-58              [-1, 512, 16]               0\n",
      "           Conv1d-59              [-1, 512, 16]       2,884,096\n",
      "      BatchNorm1d-60              [-1, 512, 16]           1,024\n",
      "             ReLU-61              [-1, 512, 16]               0\n",
      "conv1DBatchNormRelu-62              [-1, 512, 16]               0\n",
      "           Conv1d-63              [-1, 512, 16]       2,884,096\n",
      "      BatchNorm1d-64              [-1, 512, 16]           1,024\n",
      "             ReLU-65              [-1, 512, 16]               0\n",
      "conv1DBatchNormRelu-66              [-1, 512, 16]               0\n",
      "           Conv1d-67             [-1, 1024, 16]       5,768,192\n",
      "      BatchNorm1d-68             [-1, 1024, 16]           2,048\n",
      "             ReLU-69             [-1, 1024, 16]               0\n",
      "conv1DBatchNormRelu-70             [-1, 1024, 16]               0\n",
      "        MaxPool1d-71              [-1, 1024, 8]               0\n",
      "           Conv1d-72              [-1, 1024, 8]      11,535,360\n",
      "      BatchNorm1d-73              [-1, 1024, 8]           2,048\n",
      "             ReLU-74              [-1, 1024, 8]               0\n",
      "conv1DBatchNormRelu-75              [-1, 1024, 8]               0\n",
      "           Conv1d-76              [-1, 1024, 8]      11,535,360\n",
      "      BatchNorm1d-77              [-1, 1024, 8]           2,048\n",
      "             ReLU-78              [-1, 1024, 8]               0\n",
      "conv1DBatchNormRelu-79              [-1, 1024, 8]               0\n",
      "           Conv1d-80              [-1, 2048, 8]      23,070,720\n",
      "      BatchNorm1d-81              [-1, 2048, 8]           4,096\n",
      "             ReLU-82              [-1, 2048, 8]               0\n",
      "conv1DBatchNormRelu-83              [-1, 2048, 8]               0\n",
      "        MaxPool1d-84              [-1, 2048, 4]               0\n",
      "         Upsample-85              [-1, 2048, 8]               0\n",
      "           Conv1d-86              [-1, 1024, 8]      23,069,696\n",
      "      BatchNorm1d-87              [-1, 1024, 8]           2,048\n",
      "             ReLU-88              [-1, 1024, 8]               0\n",
      "conv1DBatchNormRelu-89              [-1, 1024, 8]               0\n",
      "           Conv1d-90              [-1, 1024, 8]      11,535,360\n",
      "      BatchNorm1d-91              [-1, 1024, 8]           2,048\n",
      "             ReLU-92              [-1, 1024, 8]               0\n",
      "conv1DBatchNormRelu-93              [-1, 1024, 8]               0\n",
      "         Upsample-94             [-1, 1024, 16]               0\n",
      "           Conv1d-95              [-1, 512, 16]       5,767,680\n",
      "      BatchNorm1d-96              [-1, 512, 16]           1,024\n",
      "             ReLU-97              [-1, 512, 16]               0\n",
      "conv1DBatchNormRelu-98              [-1, 512, 16]               0\n",
      "           Conv1d-99              [-1, 512, 16]       2,884,096\n",
      "     BatchNorm1d-100              [-1, 512, 16]           1,024\n",
      "            ReLU-101              [-1, 512, 16]               0\n",
      "conv1DBatchNormRelu-102              [-1, 512, 16]               0\n",
      "        Upsample-103              [-1, 512, 32]               0\n",
      "          Conv1d-104              [-1, 256, 32]       1,442,048\n",
      "     BatchNorm1d-105              [-1, 256, 32]             512\n",
      "            ReLU-106              [-1, 256, 32]               0\n",
      "conv1DBatchNormRelu-107              [-1, 256, 32]               0\n",
      "          Conv1d-108              [-1, 256, 32]         721,152\n",
      "     BatchNorm1d-109              [-1, 256, 32]             512\n",
      "            ReLU-110              [-1, 256, 32]               0\n",
      "conv1DBatchNormRelu-111              [-1, 256, 32]               0\n",
      "        Upsample-112              [-1, 256, 64]               0\n",
      "          Conv1d-113              [-1, 128, 64]         360,576\n",
      "     BatchNorm1d-114              [-1, 128, 64]             256\n",
      "            ReLU-115              [-1, 128, 64]               0\n",
      "conv1DBatchNormRelu-116              [-1, 128, 64]               0\n",
      "          Conv1d-117              [-1, 128, 64]         180,352\n",
      "     BatchNorm1d-118              [-1, 128, 64]             256\n",
      "            ReLU-119              [-1, 128, 64]               0\n",
      "conv1DBatchNormRelu-120              [-1, 128, 64]               0\n",
      "        Upsample-121             [-1, 128, 128]               0\n",
      "          Conv1d-122              [-1, 64, 128]          90,176\n",
      "     BatchNorm1d-123              [-1, 64, 128]             128\n",
      "            ReLU-124              [-1, 64, 128]               0\n",
      "conv1DBatchNormRelu-125              [-1, 64, 128]               0\n",
      "        Upsample-126              [-1, 64, 256]               0\n",
      "          Conv1d-127              [-1, 32, 256]          22,560\n",
      "     BatchNorm1d-128              [-1, 32, 256]              64\n",
      "            ReLU-129              [-1, 32, 256]               0\n",
      "conv1DBatchNormRelu-130              [-1, 32, 256]               0\n",
      "        Upsample-131              [-1, 32, 512]               0\n",
      "          Conv1d-132              [-1, 16, 512]           5,648\n",
      "     BatchNorm1d-133              [-1, 16, 512]              32\n",
      "            ReLU-134              [-1, 16, 512]               0\n",
      "conv1DBatchNormRelu-135              [-1, 16, 512]               0\n",
      "          Conv1d-136               [-1, 1, 508]              81\n",
      "================================================================\n",
      "Total params: 107,565,898\n",
      "Trainable params: 107,565,898\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 10.59\n",
      "Params size (MB): 410.33\n",
      "Estimated Total Size (MB): 420.92\n",
      "----------------------------------------------------------------\n",
      "Element type                                            Size  Used MEM\n",
      "-------------------------------------------------------------------------------\n",
      "Storage on cuda:0\n",
      "Tensor0                                                 (1,)   512.00B\n",
      "Tensor1                                                 (1,)   512.00B\n",
      "Tensor2                                                 (1,)   512.00B\n",
      "Tensor3                                                 (1,)   512.00B\n",
      "Tensor4                                                 (1,)   512.00B\n",
      "Tensor5                                                 (1,)   512.00B\n",
      "Tensor6                                                 (1,)   512.00B\n",
      "Tensor7                                                 (1,)   512.00B\n",
      "Tensor8                                                 (1,)   512.00B\n",
      "Tensor9                                                 (1,)   512.00B\n",
      "Tensor10                                                (1,)   512.00B\n",
      "Tensor11                                                (1,)   512.00B\n",
      "Tensor12                                                (1,)   512.00B\n",
      "Tensor13                                                (1,)   512.00B\n",
      "Tensor14                                                (1,)   512.00B\n",
      "Tensor15                                                (1,)   512.00B\n",
      "Tensor16                                                (1,)   512.00B\n",
      "Tensor17                                                (1,)   512.00B\n",
      "Tensor18                                                (1,)   512.00B\n",
      "Tensor19                                                (1,)   512.00B\n",
      "Tensor20                                                (1,)   512.00B\n",
      "embedding.weight                                      (5, 5)   512.00B\n",
      "convs.0.conv.weight                              (16, 5, 11)     3.50K\n",
      "convs.0.conv.bias                                      (16,)   512.00B\n",
      "convs.0.batchnorm.weight                               (16,)   512.00B\n",
      "convs.0.batchnorm.bias                                 (16,)   512.00B\n",
      "convs.1.conv.weight                             (16, 16, 11)    11.00K\n",
      "convs.1.conv.bias                                      (16,)   512.00B\n",
      "convs.1.batchnorm.weight                               (16,)   512.00B\n",
      "convs.1.batchnorm.bias                                 (16,)   512.00B\n",
      "convs.2.conv.weight                             (32, 16, 11)    22.00K\n",
      "convs.2.conv.bias                                      (32,)   512.00B\n",
      "convs.2.batchnorm.weight                               (32,)   512.00B\n",
      "convs.2.batchnorm.bias                                 (32,)   512.00B\n",
      "convs.4.conv.weight                             (32, 32, 11)    44.00K\n",
      "convs.4.conv.bias                                      (32,)   512.00B\n",
      "convs.4.batchnorm.weight                               (32,)   512.00B\n",
      "convs.4.batchnorm.bias                                 (32,)   512.00B\n",
      "convs.5.conv.weight                             (64, 32, 11)    88.00K\n",
      "convs.5.conv.bias                                      (64,)   512.00B\n",
      "convs.5.batchnorm.weight                               (64,)   512.00B\n",
      "convs.5.batchnorm.bias                                 (64,)   512.00B\n",
      "convs.7.conv.weight                             (64, 64, 11)   176.00K\n",
      "convs.7.conv.bias                                      (64,)   512.00B\n",
      "convs.7.batchnorm.weight                               (64,)   512.00B\n",
      "convs.7.batchnorm.bias                                 (64,)   512.00B\n",
      "convs.8.conv.weight                            (128, 64, 11)   352.00K\n",
      "convs.8.conv.bias                                     (128,)   512.00B\n",
      "convs.8.batchnorm.weight                              (128,)   512.00B\n",
      "convs.8.batchnorm.bias                                (128,)   512.00B\n",
      "convs.10.conv.weight                          (128, 128, 11)   704.00K\n",
      "convs.10.conv.bias                                    (128,)   512.00B\n",
      "convs.10.batchnorm.weight                             (128,)   512.00B\n",
      "convs.10.batchnorm.bias                               (128,)   512.00B\n",
      "convs.11.conv.weight                          (128, 128, 11)   704.00K\n",
      "convs.11.conv.bias                                    (128,)   512.00B\n",
      "convs.11.batchnorm.weight                             (128,)   512.00B\n",
      "convs.11.batchnorm.bias                               (128,)   512.00B\n",
      "convs.12.conv.weight                          (256, 128, 11)     1.38M\n",
      "convs.12.conv.bias                                    (256,)     1.00K\n",
      "convs.28.batchnorm.weight                            (1024,)     4.00K\n",
      "convs.12.batchnorm.weight                             (256,)     1.00K\n",
      "convs.12.batchnorm.bias                               (256,)     1.00K\n",
      "convs.14.conv.weight                          (256, 256, 11)     2.75M\n",
      "convs.14.conv.bias                                    (256,)     1.00K\n",
      "convs.14.batchnorm.weight                             (256,)     1.00K\n",
      "convs.14.batchnorm.bias                               (256,)     1.00K\n",
      "convs.15.conv.weight                          (256, 256, 11)     2.75M\n",
      "convs.15.conv.bias                                    (256,)     1.00K\n",
      "convs.15.batchnorm.weight                             (256,)     1.00K\n",
      "convs.15.batchnorm.bias                               (256,)     1.00K\n",
      "convs.16.conv.weight                          (512, 256, 11)     5.50M\n",
      "convs.16.conv.bias                                    (512,)     2.00K\n",
      "convs.16.batchnorm.weight                             (512,)     2.00K\n",
      "convs.16.batchnorm.bias                               (512,)     2.00K\n",
      "convs.18.conv.weight                          (512, 512, 11)    11.00M\n",
      "convs.18.conv.bias                                    (512,)     2.00K\n",
      "convs.18.batchnorm.weight                             (512,)     2.00K\n",
      "convs.18.batchnorm.bias                               (512,)     2.00K\n",
      "convs.19.conv.weight                          (512, 512, 11)    11.00M\n",
      "convs.19.conv.bias                                    (512,)     2.00K\n",
      "convs.19.batchnorm.weight                             (512,)     2.00K\n",
      "convs.19.batchnorm.bias                               (512,)     2.00K\n",
      "convs.20.conv.weight                         (1024, 512, 11)    22.00M\n",
      "convs.20.conv.bias                                   (1024,)     4.00K\n",
      "convs.20.batchnorm.weight                            (1024,)     4.00K\n",
      "convs.20.batchnorm.bias                              (1024,)     4.00K\n",
      "convs.22.conv.weight                        (1024, 1024, 11)    44.00M\n",
      "convs.22.conv.bias                                   (1024,)     4.00K\n",
      "convs.22.batchnorm.weight                            (1024,)     4.00K\n",
      "convs.22.batchnorm.bias                              (1024,)     4.00K\n",
      "convs.23.conv.weight                        (1024, 1024, 11)    44.00M\n",
      "convs.23.conv.bias                                   (1024,)     4.00K\n",
      "convs.23.batchnorm.weight                            (1024,)     4.00K\n",
      "convs.23.batchnorm.bias                              (1024,)     4.00K\n",
      "convs.24.conv.weight                        (2048, 1024, 11)    88.00M\n",
      "convs.24.conv.bias                                   (2048,)     8.00K\n",
      "convs.24.batchnorm.weight                            (2048,)     8.00K\n",
      "convs.24.batchnorm.bias                              (2048,)     8.00K\n",
      "convs.27.conv.weight                        (1024, 2048, 11)    88.00M\n",
      "convs.27.conv.bias                                   (1024,)     4.00K\n",
      "convs.27.batchnorm.weight                            (1024,)     4.00K\n",
      "convs.27.batchnorm.bias                              (1024,)     4.00K\n",
      "convs.28.conv.weight                        (1024, 1024, 11)    44.00M\n",
      "convs.28.conv.bias                                   (1024,)     4.00K\n",
      "convs.28.batchnorm.bias                              (1024,)     4.00K\n",
      "Tensor21                                               (16,)   512.00B\n",
      "Tensor22                                               (16,)   512.00B\n",
      "Tensor23                                               (16,)   512.00B\n",
      "Tensor24                                               (16,)   512.00B\n",
      "Tensor25                                               (32,)   512.00B\n",
      "Tensor26                                               (32,)   512.00B\n",
      "Tensor27                                               (32,)   512.00B\n",
      "Tensor28                                               (32,)   512.00B\n",
      "Tensor29                                               (64,)   512.00B\n",
      "Tensor30                                               (64,)   512.00B\n",
      "Tensor31                                               (64,)   512.00B\n",
      "Tensor32                                               (64,)   512.00B\n",
      "Tensor33                                              (128,)   512.00B\n",
      "Tensor34                                              (128,)   512.00B\n",
      "Tensor35                                              (128,)   512.00B\n",
      "Tensor36                                              (128,)   512.00B\n",
      "Tensor37                                              (128,)   512.00B\n",
      "Tensor38                                              (128,)   512.00B\n",
      "Tensor39                                              (256,)     1.00K\n",
      "Tensor40                                              (256,)     1.00K\n",
      "Tensor41                                              (256,)     1.00K\n",
      "Tensor42                                              (256,)     1.00K\n",
      "Tensor43                                              (256,)     1.00K\n",
      "Tensor44                                              (256,)     1.00K\n",
      "Tensor45                                              (512,)     2.00K\n",
      "Tensor46                                              (512,)     2.00K\n",
      "Tensor47                                              (512,)     2.00K\n",
      "Tensor48                                              (512,)     2.00K\n",
      "Tensor49                                              (512,)     2.00K\n",
      "Tensor50                                              (512,)     2.00K\n",
      "Tensor51                                             (1024,)     4.00K\n",
      "Tensor52                                             (1024,)     4.00K\n",
      "Tensor53                                             (1024,)     4.00K\n",
      "Tensor54                                             (1024,)     4.00K\n",
      "Tensor55                                             (1024,)     4.00K\n",
      "Tensor56                                             (1024,)     4.00K\n",
      "Tensor57                                             (2048,)     8.00K\n",
      "Tensor58                                             (2048,)     8.00K\n",
      "Tensor59                                             (1024,)     4.00K\n",
      "Tensor60                                             (1024,)     4.00K\n",
      "Tensor61                                             (1024,)     4.00K\n",
      "Tensor62                                             (1024,)     4.00K\n",
      "Tensor63                                              (512,)     2.00K\n",
      "Tensor64                                              (512,)     2.00K\n",
      "Tensor65                                              (512,)     2.00K\n",
      "Tensor66                                              (512,)     2.00K\n",
      "Tensor67                                              (256,)     1.00K\n",
      "Tensor68                                              (256,)     1.00K\n",
      "Tensor69                                              (256,)     1.00K\n",
      "Tensor70                                              (256,)     1.00K\n",
      "Tensor71                                              (128,)   512.00B\n",
      "Tensor72                                              (128,)   512.00B\n",
      "Tensor73                                              (128,)   512.00B\n",
      "Tensor74                                              (128,)   512.00B\n",
      "Tensor75                                               (64,)   512.00B\n",
      "Tensor76                                               (64,)   512.00B\n",
      "Tensor77                                               (32,)   512.00B\n",
      "Tensor78                                               (32,)   512.00B\n",
      "Tensor79                                               (16,)   512.00B\n",
      "Tensor80                                               (16,)   512.00B\n",
      "convs.30.conv.weight                         (512, 1024, 11)    22.00M\n",
      "convs.30.conv.bias                                    (512,)     2.00K\n",
      "convs.30.batchnorm.weight                             (512,)     2.00K\n",
      "convs.30.batchnorm.bias                               (512,)     2.00K\n",
      "convs.31.conv.weight                          (512, 512, 11)    11.00M\n",
      "convs.31.conv.bias                                    (512,)     2.00K\n",
      "convs.31.batchnorm.weight                             (512,)     2.00K\n",
      "convs.31.batchnorm.bias                               (512,)     2.00K\n",
      "convs.33.conv.weight                          (256, 512, 11)     5.50M\n",
      "convs.33.conv.bias                                    (256,)     1.00K\n",
      "convs.33.batchnorm.weight                             (256,)     1.00K\n",
      "convs.33.batchnorm.bias                               (256,)     1.00K\n",
      "convs.34.conv.weight                          (256, 256, 11)     2.75M\n",
      "convs.34.conv.bias                                    (256,)     1.00K\n",
      "convs.34.batchnorm.weight                             (256,)     1.00K\n",
      "convs.34.batchnorm.bias                               (256,)     1.00K\n",
      "convs.36.conv.weight                          (128, 256, 11)     1.38M\n",
      "convs.36.conv.bias                                    (128,)   512.00B\n",
      "convs.36.batchnorm.weight                             (128,)   512.00B\n",
      "convs.36.batchnorm.bias                               (128,)   512.00B\n",
      "convs.37.conv.weight                          (128, 128, 11)   704.00K\n",
      "convs.37.conv.bias                                    (128,)   512.00B\n",
      "convs.37.batchnorm.weight                             (128,)   512.00B\n",
      "convs.37.batchnorm.bias                               (128,)   512.00B\n",
      "convs.39.conv.weight                           (64, 128, 11)   352.00K\n",
      "convs.39.conv.bias                                     (64,)   512.00B\n",
      "convs.39.batchnorm.weight                              (64,)   512.00B\n",
      "convs.39.batchnorm.bias                                (64,)   512.00B\n",
      "convs.41.conv.weight                            (32, 64, 11)    88.00K\n",
      "convs.41.conv.bias                                     (32,)   512.00B\n",
      "convs.41.batchnorm.weight                              (32,)   512.00B\n",
      "convs.41.batchnorm.bias                                (32,)   512.00B\n",
      "convs.43.conv.weight                            (16, 32, 11)    22.00K\n",
      "convs.43.conv.bias                                     (16,)   512.00B\n",
      "convs.43.batchnorm.weight                              (16,)   512.00B\n",
      "convs.43.batchnorm.bias                                (16,)   512.00B\n",
      "fc.weight                                         (1, 16, 5)   512.00B\n",
      "fc.bias                                                 (1,)   512.00B\n",
      "Tensor81                                                (1,)   512.00B\n",
      "Tensor82                                                (1,)   512.00B\n",
      "Tensor83                                                (1,)   512.00B\n",
      "Tensor84                                                (1,)   512.00B\n",
      "Tensor85                                                (1,)   512.00B\n",
      "Tensor86                                                (1,)   512.00B\n",
      "Tensor87                                                (1,)   512.00B\n",
      "Tensor88                                                (1,)   512.00B\n",
      "Tensor89                                                (1,)   512.00B\n",
      "-------------------------------------------------------------------------------\n",
      "Total Tensors: 107589896 \tUsed Memory: 410.45M\n",
      "The allocated memory on cuda:0: 414.20M\n",
      "Memory differs due to the matrix alignment or invisible gradient buffer tensors\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Storage on cpu\n",
      "Tensor90                                      (2000000, 512)     3.81G\n",
      "Tensor91                                      (2000000, 508)     3.78G\n",
      "-------------------------------------------------------------------------------\n",
      "Total Tensors: 2040000000 \tUsed Memory: 7.60G\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss:2.7162 Timer:5316.3958\n",
      "val Loss:2.5558 Timer:216.4194\n",
      "Epoch 2/20\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "dataloaders_dict = {'train': train_dataloader, 'val': val_dataloader}\n",
    "\n",
    "for n in [0]:\n",
    "    for x in [0]:\n",
    "        datePrint(n, 'nani', x, 'layer')\n",
    "        net = model.new_Variable_3(num_layer=8, num_filters=128, kernel_sizes=11).to(device)\n",
    "        net.apply(model.weight_init) #重みの初期化適用\n",
    "        summary(net, (512,))\n",
    "        reporter = MemReporter(net)\n",
    "        reporter.report()\n",
    "        #ファインチューニング\n",
    "#         optimizer = optim.Adam([{'params': net.embedding.parameters(), 'lr': 5e-4},\n",
    "#                                 {'params': net.convs.parameters(), 'lr': 1e-4},\n",
    "#                                 {'params': net.mid.parameters(), 'lr': 5e-4},\n",
    "#                                 {'params': net.fc.parameters(), 'lr': 1e-3}], weight_decay=1e-6)\n",
    "        optimizer = optim.Adam(net.parameters(), lr=1e-4, weight_decay=1e-6, eps=1e-5)\n",
    "        epochs = 20\n",
    "        criterion = nn.MSELoss().to(device)\n",
    "#         # 学習途中データ\n",
    "#         checkpoint = torch.load('max_span100.pth')\n",
    "#         net.load_state_dict(checkpoint['model_state_dict'])\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         epochs = checkpoint['epoch']\n",
    "#         loss = checkpoint['loss']\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)\n",
    "        train_loss_list, val_loss_list, data_all, target_all, output_all = mode.train(device, net, dataloaders_dict, criterion, optimizer, epochs)               \n",
    "#         torch.save(net.state_dict(), 'max_span100.pth')\n",
    "    \n",
    "        print(f'memory report')\n",
    "#         reporter.report()\n",
    "        \n",
    "        result.learning_curve(train_loss_list, val_loss_list, epochs)\n",
    "        result.plot_result(np.array(target_all, dtype=object).reshape(-1), np.array(output_all, dtype=object).reshape(-1))\n",
    "        cor_list, loss_list = result.cal_indicators(target_all, output_all)\n",
    "        result.cor_hist(cor_list)\n",
    "        result.scatter_minmax(cor_list, loss_list, target_all, output_all)\n",
    "        result.visible_minmax(target_all, output_all, cor_list, loss_list)\n",
    "        result.show_base(data_all, cor_list, loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  poolingで減らしてますよん"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    result.visible_one(target_all, output_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_test = pickle.load(open(\"../data/test_sets/input_600_1000.pkl\",\"rb\"))\n",
    "# target_test = pickle.load(open(\"../data/test_sets/target_600_1000.pkl\",\"rb\"))\n",
    "input_test = pickle.load(open(\"../data/RF00156/input_RF00156.pkl\",\"rb\"))\n",
    "target_test = pickle.load(open(\"../data/RF00156/target_RF00156.pkl\",\"rb\"))\n",
    "target_test = torch.flip(target_test, dims=[1])\n",
    "datePrint(input_test.shape)\n",
    "datePrint(target_test.shape)\n",
    "transform = False\n",
    "# if (input_test.shape[1]%256 != 0):\n",
    "#     input_test = F.pad(input_test, (0, 256-input_test.shape[1]%256))\n",
    "#     target_test = F.pad(target_test, (0, 252-target_test.shape[1]%256))\n",
    "# if (input_test.shape[1]>256):\n",
    "#     transform = True\n",
    "#     division = (input_test.shape[1])//128 - 1\n",
    "#     input_init = input_test\n",
    "#     input_test = input_test.unfold(1, 256, 128).reshape(-1, 256)\n",
    "#     target_test = target_test.unfold(1, 252, 128).reshape(-1, 252)\n",
    "# else:\n",
    "#     transform = False\n",
    "# datePrint(input_test.shape)\n",
    "# datePrint(target_test.shape)\n",
    "test_dataset = model.Dataset(input_test, target_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64,shuffle=False, num_workers=1)\n",
    "\n",
    "net = model.Variable(num_layer=16, kernel_sizes=33, flag=False).to(device)\n",
    "net.load_state_dict(torch.load('max_span20.pth'))\n",
    "criterion = nn.MSELoss().to(device)\n",
    "\n",
    "start = time.time()\n",
    "test_loss, data_all, target_all, output_all = mode.test(device, net, test_dataloader, criterion) \n",
    "datePrint('finish prediction loss', test_loss)\n",
    "if (transform==True):\n",
    "    target_tmp = torch.tensor(target_all)\n",
    "    output_tmp = torch.tensor(output_all)\n",
    "    output_tmp = F.relu(output_tmp)\n",
    "    for n in range(division):\n",
    "        if (n==0):\n",
    "            target_all = target_tmp[n::division, :192]\n",
    "            output_all = output_tmp[n::division, :192]\n",
    "        elif (n==division-1):\n",
    "            target_all = torch.cat([target_all, target_tmp[n::division, 64:]], dim=1)\n",
    "            output_all = torch.cat([output_all, output_tmp[n::division, 64:]], dim=1)\n",
    "        else:\n",
    "            target_all = torch.cat([target_all, target_tmp[n::division, 64:192]], dim=1)\n",
    "            output_all = torch.cat([output_all, output_tmp[n::division, 64:192]], dim=1)\n",
    "    \n",
    "    data_all = input_init.numpy()\n",
    "    target_all = target_all.numpy()\n",
    "    output_all = output_all.numpy()\n",
    "finish = time.time()\n",
    "np.savetxt('accessibility_output.txt', output_all, fmt='%.3e')\n",
    "finish = time.time()\n",
    "datePrint('予測時間', (finish-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datePrint('test_loss: {:.3f}'.format(test_loss))\n",
    "result.plot_result(np.array(target_all).reshape(-1), np.array(output_all).reshape(-1))\n",
    "cor_list, loss_list = result.cal_indicators(target_all, output_all)\n",
    "result.cor_hist(cor_list)\n",
    "result.scatter_minmax(cor_list, loss_list, target_all, output_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.visible_minmax(target_all, output_all, cor_list, loss_list)\n",
    "result.show_base(data_all, cor_list, loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    result.visible_one(target_all, output_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "path = \"../data/human_data/seq7.fa\"\n",
    "with open(path, mode = 'r', encoding = 'utf-8') as f:\n",
    "    seq_file = f.read().splitlines()\n",
    "indexes = [i for i, n in enumerate(seq_file) if n.startswith('>')]\n",
    "name = seq_file[0]\n",
    "seq = ''.join(seq_file[1:]).replace('A', '1').replace('T', '2').replace('U', '2').replace('G', '3').replace('C', '4')\n",
    "input_seq = torch.Tensor(list(map(int, seq)))\n",
    "input_seq = torch.flip(input_seq, dims=[0])\n",
    "input_seq = input_seq.unsqueeze(0)\n",
    "out_length = len(input_seq[0])-4\n",
    "\n",
    "transform = False\n",
    "# if (input_seq.shape[1]%256 != 0):\n",
    "#     input_seq = F.pad(input_seq, (0, 256-input_seq.shape[1]%256))\n",
    "# if (input_seq.shape[1]>256):\n",
    "#     transform = True\n",
    "#     division = (input_seq.shape[1])//128 - 1\n",
    "#     input_init = input_seq\n",
    "#     input_seq = input_seq.unfold(1, 256, 128).reshape(-1, 256)\n",
    "# else:\n",
    "#     transform = False\n",
    "input_seq = input_seq.unsqueeze(0)\n",
    "\n",
    "net = model.Variable(num_layer=16, kernel_sizes=33, flag=False).to(device)\n",
    "net.load_state_dict(torch.load('max_span20.pth'))\n",
    "    \n",
    "data_all, output_all = mode.predict(device, net, input_seq) \n",
    "\n",
    "if (transform==True):\n",
    "    output_tmp = torch.tensor(output_all)\n",
    "    for n in range(division):\n",
    "        if (n==0):\n",
    "            output_all = output_tmp[n::division, :192]\n",
    "        elif (n==division-1):\n",
    "            output_all = torch.cat([output_all, output_tmp[n::division, 64:]], dim=1)\n",
    "        else:\n",
    "            output_all = torch.cat([output_all, output_tmp[n::division, 64:192]], dim=1)\n",
    "    \n",
    "    data_all = input_init.numpy()\n",
    "#     output_all = torch.flip(output_all, dims=[1])\n",
    "    output_all = output_all.numpy()\n",
    "  \n",
    "import matplotlib.pyplot as plt\n",
    "max_length = output_all.shape[1]\n",
    "with open('../data/human_data/out7.txt', 'r') as f:\n",
    "        next(f)\n",
    "        acc = f.readlines()\n",
    "        acc_list = []\n",
    "        for i in range(len(acc)-1):\n",
    "                acc_list.append(re.findall(',(.*);', acc[i])[0])\n",
    "acc_list = [float(x) for x in acc_list]\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(range(out_length), acc_list, label='target', color='b')\n",
    "plt.plot(range(out_length), output_all[0][:out_length], label='output', color='r')\n",
    "plt.legend()\n",
    "plt.xlabel('base position')\n",
    "plt.ylabel('accessibility')\n",
    "plt.title('one')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cor = np.corrcoef(acc_list, output_all[0][:out_length])\n",
    "mse = ((acc_list - output_all[0][:out_length])**2).mean(axis=0)\n",
    "print('cor', cor[0,1])\n",
    "print('mse', mse)\n",
    "\n",
    "    \n",
    "np.savetxt('accessibility_output.txt', output_all, fmt='%.3f')\n",
    "finish = time.time()\n",
    "datePrint('予測時間', (finish-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/real_data/RF00156.fa\"\n",
    "\n",
    "start = time.time()\n",
    "with open(path, mode = 'r', encoding = 'utf-8') as f:\n",
    "    seq_file = f.read().splitlines()\n",
    "indexes = [i for i, n in enumerate(seq_file) if n.startswith('>')]\n",
    "name_list = []\n",
    "seq_list = torch.empty(0, 256)\n",
    "\n",
    "for i in range(len(indexes)-1):\n",
    "    name_list.append(seq_file[indexes[i]])\n",
    "    seq = ''.join(seq_file[indexes[i]+1:indexes[i+1]]).replace('A', '1').replace('T', '2').replace('U', '2').replace('G', '3').replace('C', '4').replace('N', '0')\n",
    "    seq = torch.Tensor(list(map(int, seq)))\n",
    "    seq = torch.flip(seq, dims=[0])\n",
    "    if (seq.shape[0]%256 != 0):\n",
    "        seq = F.pad(seq, (0, 256-seq.shape[0]%256))\n",
    "    seq = seq.unsqueeze(0)\n",
    "    seq_list = torch.cat([seq_list, seq], dim=0)\n",
    "\n",
    "\n",
    "# if (input_seq.shape[1]%256 != 0):\n",
    "#     input_seq = F.pad(input_seq, (0, 256-input_seq.shape[1]%256))\n",
    "# if (input_seq.shape[1]>256):\n",
    "#     transform = True\n",
    "#     division = (input_seq.shape[1])//128 - 1\n",
    "#     input_init = input_seq\n",
    "#     input_seq = input_seq.unfold(1, 256, 128).reshape(-1, 256)\n",
    "# else:\n",
    "#     transform = False\n",
    "# datePrint(input_seq.shape)\n",
    "# input_seq = input_seq.unsqueeze(0)\n",
    "# datePrint(input_seq.shape)\n",
    "\n",
    "seq_list = seq_list.unsqueeze(0)\n",
    "net = model.Variable(num_layer=16, kernel_sizes=33, flag=False).to(device)\n",
    "net.load_state_dict(torch.load('max_span20.pth'))\n",
    "    \n",
    "data_all, output_all = mode.predict(device, net, seq_list) \n",
    "# if (transform==True):\n",
    "#     output_tmp = torch.tensor(output_all)\n",
    "#     for n in range(division):\n",
    "#         if (n==0):\n",
    "#             output_all = output_tmp[n::division, :192]\n",
    "#         elif (n==division-1):\n",
    "#             output_all = torch.cat([output_all, output_tmp[n::division, 64:]], dim=1)\n",
    "#         else:\n",
    "#             output_all = torch.cat([output_all, output_tmp[n::division, 64:192]], dim=1)\n",
    "    \n",
    "#     data_all = input_init.numpy()\n",
    "#     output_all = output_all.numpy()\n",
    "    \n",
    "np.savetxt('accessibility_output.txt', output_all, fmt='%.3f')\n",
    "finish = time.time()\n",
    "datePrint('予測時間', (finish-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample作成\n",
    "path = \"../data/RF01210/RF01210.fa\"\n",
    "\n",
    "with open(path, mode = 'r', encoding = 'utf-8') as f:\n",
    "    seq_file = f.read().splitlines()\n",
    "indexes = [i for i, n in enumerate(seq_file) if n.startswith('>')]\n",
    "\n",
    "for i in range(len(indexes)-1):\n",
    "    with open(f\"../data/RF01210/sample_{i}.txt\", mode=\"w\") as f:\n",
    "        f.write(seq_file[indexes[i]] + \"\\n\")\n",
    "        f.write(''.join(seq_file[indexes[i]+1:indexes[i+1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# castしちゃう\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "for cond, cnt in ((\"train\", 500000), (\"test\", 500000)):\n",
    "    data_path = Path(f\"../data/makedata/{cond}\")\n",
    "    input_array = []\n",
    "    target_array = []\n",
    "    print(f\"reading {cond} files\")\n",
    "    for i in tqdm(range(cnt)):\n",
    "        input_path = data_path / f\"index/input_{i+1}.csv\"\n",
    "        target_path = data_path / f\"accessibility/target_{i+1}.csv\"\n",
    "\n",
    "        input_array.append(torch.Tensor(np.loadtxt(input_path, delimiter=\",\", dtype=np.float).astype(np.int)))\n",
    "        target_array.append(torch.Tensor(np.loadtxt(target_path, delimiter=\",\", dtype=np.float)))\n",
    "    print(f\"saving to input_{cond}.pkl\")\n",
    "    pickle.dump(torch.stack(input_array), open(f\"../data/input_{cond}.pkl\", 'wb'))\n",
    "        \n",
    "    print(f\"saving to target_{cond}.pkl\")\n",
    "    pickle.dump(torch.stack(target_array), open(f\"../data/target_{cond}.pkl\", 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4iN96X35s5E8"
   },
   "outputs": [],
   "source": [
    "#メモリ確認\n",
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 10000: #10M以上のみ表示\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN.ipynb",
   "provenance": [
    {
     "file_id": "1ir0IJd6vc8l_FMQY9-YSFWxAQiIxZF6p",
     "timestamp": 1591723758584
    },
    {
     "file_id": "1cSw8tGp7_Atz1RUom-WbFEdJ1yVDgKCC",
     "timestamp": 1591700460320
    },
    {
     "file_id": "1-iiGLGF3JfbxpJ5cz5GyMTmbOMxhDH6D",
     "timestamp": 1591434878746
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "16b98715895a42739075b7711e0ba437": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ee0ab1bbc1e4cada381cdadbc3dc5f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c5b3c652eab41459f3779503ab44717": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16b98715895a42739075b7711e0ba437",
      "placeholder": "​",
      "style": "IPY_MODEL_2ee0ab1bbc1e4cada381cdadbc3dc5f2",
      "value": " 0/50 [00:00&lt;?, ?it/s]"
     }
    },
    "4d55df15f3f947888809d424885dfe55": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a817f317b5c42f592f130f576ee3063",
       "IPY_MODEL_3c5b3c652eab41459f3779503ab44717"
      ],
      "layout": "IPY_MODEL_a3ed74bd975e4e87955a0de33c520916"
     }
    },
    "5098cf4e3a6a4e5fb9daec2f6dc59c9d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a817f317b5c42f592f130f576ee3063": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "  0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5098cf4e3a6a4e5fb9daec2f6dc59c9d",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de9d1fa4cee24559a816afe706b91081",
      "value": 0
     }
    },
    "a3ed74bd975e4e87955a0de33c520916": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de9d1fa4cee24559a816afe706b91081": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
