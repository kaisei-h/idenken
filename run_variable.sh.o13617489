Warning: no access to tty (Bad file descriptor).
Thus no job control in this shell.
[2021/08/05 23:44:20] loading pickle data
[5000000, 500000]
dilation1(
  (embedding): Embedding(5, 5)
  (convs): ModuleList(
    (0): conv1DBatchNorm(
      (conv): Conv1d(5, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): Hardswish()
    (2): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(6,), dilation=(3,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(10,), dilation=(5,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(14,), dilation=(7,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (6): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (7): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(6,), dilation=(3,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (8): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(10,), dilation=(5,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (9): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(14,), dilation=(7,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (10): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (11): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(6,), dilation=(3,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (12): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(10,), dilation=(5,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (13): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(14,), dilation=(7,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (14): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (15): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(6,), dilation=(3,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (16): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(10,), dilation=(5,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (17): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(14,), dilation=(7,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (18): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (19): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(6,), dilation=(3,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (20): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(10,), dilation=(5,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (21): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(14,), dilation=(7,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (22): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (23): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(6,), dilation=(3,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (24): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(10,), dilation=(5,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (25): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(14,), dilation=(7,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (26): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (27): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(6,), dilation=(3,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (28): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(10,), dilation=(5,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (29): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(14,), dilation=(7,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (30): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (31): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(6,), dilation=(3,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (32): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(10,), dilation=(5,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (33): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(14,), dilation=(7,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (34): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (35): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(6,), dilation=(3,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (36): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(10,), dilation=(5,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (37): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(14,), dilation=(7,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (38): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (39): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(6,), dilation=(3,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (40): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(10,), dilation=(5,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (41): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(14,), dilation=(7,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (42): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (43): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(6,), dilation=(3,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (44): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(10,), dilation=(5,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (45): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(14,), dilation=(7,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (46): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (47): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(6,), dilation=(3,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (48): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(10,), dilation=(5,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (49): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(14,), dilation=(7,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (50): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (51): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(6,), dilation=(3,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (52): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(10,), dilation=(5,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (53): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(14,), dilation=(7,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (54): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (55): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(6,), dilation=(3,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (56): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(10,), dilation=(5,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (57): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(14,), dilation=(7,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (58): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (59): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(6,), dilation=(3,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (60): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(10,), dilation=(5,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (61): conv1DBatchNormRelu(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(14,), dilation=(7,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (62): conv1DBatchNorm(
      (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (63): Hardswish()
    (64): conv1DBatchNorm(
      (conv): Conv1d(128, 1, kernel_size=(5,), stride=(1,), bias=False)
      (batchnorm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-05
    lr: 0.0001
    weight_decay: 1e-08
)
dilation = 1,3,5,7
Epoch 1/20
train Loss:2.4962 Timer:11189.3324
val Loss:1.7956 Timer:200.9790
Epoch 2/20
train Loss:1.7374 Timer:10845.5868
val Loss:1.6924 Timer:194.7895
Epoch 3/20
train Loss:1.6690 Timer:10911.4058
val Loss:1.6442 Timer:206.5234
Epoch 4/20
train Loss:1.6327 Timer:10733.7425
val Loss:1.6197 Timer:211.9585
Epoch 5/20
train Loss:1.6102 Timer:10862.2688
val Loss:1.6081 Timer:364.8385
Epoch 6/20
train Loss:1.5942 Timer:11465.4822
val Loss:1.5879 Timer:199.6580
Epoch 7/20
train Loss:1.5819 Timer:11781.9993
val Loss:1.5757 Timer:199.9935
Epoch 8/20
train Loss:1.5722 Timer:11350.1802
val Loss:1.5717 Timer:232.7437
Epoch 9/20
train Loss:1.5642 Timer:10890.1581
val Loss:1.5616 Timer:198.2279
Epoch 10/20
train Loss:1.5576 Timer:11178.0919
val Loss:1.5564 Timer:331.6148
Epoch 11/20
train Loss:1.5518 Timer:11399.9240
val Loss:1.5511 Timer:205.3557
Epoch 12/20
train Loss:1.5468 Timer:11944.9942
val Loss:1.5530 Timer:196.1494
Epoch 13/20
train Loss:1.5422 Timer:11034.0338
val Loss:1.5493 Timer:205.2368
Epoch 14/20
train Loss:1.5380 Timer:11542.3713
val Loss:1.5456 Timer:202.7873
Epoch 15/20
train Loss:1.5338 Timer:11124.2503
val Loss:1.5406 Timer:255.4296
Epoch 16/20
train Loss:1.5292 Timer:11021.1332
val Loss:1.5408 Timer:189.9289
Epoch 17/20
train Loss:1.5241 Timer:10706.5481
val Loss:1.5366 Timer:196.5468
Epoch 18/20
train Loss:1.5178 Timer:10713.2460
val Loss:1.5357 Timer:192.2584
Epoch 19/20
train Loss:1.5105 Timer:10554.5634
val Loss:1.5349 Timer:192.9872
Epoch 20/20
train Loss:1.5018 Timer:10480.6743
val Loss:1.5353 Timer:245.3122
